diff --git a/python/tflite_micro/python_ops_resolver.cc b/python/tflite_micro/python_ops_resolver.cc
index ab67bd4a..127c4ee9 100644
--- a/python/tflite_micro/python_ops_resolver.cc
+++ b/python/tflite_micro/python_ops_resolver.cc
@@ -39,28 +39,28 @@ PythonOpsResolver::PythonOpsResolver() {
   AddConv2D();
   AddCos();
   AddCumSum();
-  AddDelay();
+  //AddDelay();
   AddDepthToSpace();
   AddDepthwiseConv2D();
   AddDequantize();
   AddDetectionPostprocess();
   AddDiv();
-  AddEnergy();
+  //AddEnergy();
   AddElu();
   AddEqual();
   AddEthosU();
   AddExp();
   AddExpandDims();
-  AddFftAutoScale();
+  //AddFftAutoScale();
   AddFill();
-  AddFilterBank();
-  AddFilterBankLog();
-  AddFilterBankSquareRoot();
-  AddFilterBankSpectralSubtraction();
+  // AddFilterBank();
+  // AddFilterBankLog();
+  // AddFilterBankSquareRoot();
+  // AddFilterBankSpectralSubtraction();
   AddFloor();
   AddFloorDiv();
   AddFloorMod();
-  AddFramer();
+  //AddFramer();
   AddFullyConnected();
   AddGather();
   AddGatherNd();
@@ -68,7 +68,7 @@ PythonOpsResolver::PythonOpsResolver() {
   AddGreaterEqual();
   AddHardSwish();
   AddIf();
-  AddIrfft();
+  //AddIrfft();
   AddL2Normalization();
   AddL2Pool2D();
   AddLeakyRelu();
@@ -88,11 +88,11 @@ PythonOpsResolver::PythonOpsResolver() {
   AddMul();
   AddNeg();
   AddNotEqual();
-  AddOverlapAdd();
+  //AddOverlapAdd();
   AddPack();
   AddPad();
   AddPadV2();
-  AddPCAN();
+  //AddPCAN();
   AddPrelu();
   AddQuantize();
   AddReadVariable();
@@ -102,7 +102,7 @@ PythonOpsResolver::PythonOpsResolver() {
   AddReshape();
   AddResizeBilinear();
   AddResizeNearestNeighbor();
-  AddRfft();
+  //AddRfft();
   AddRound();
   AddRsqrt();
   AddSelectV2();
@@ -119,7 +119,7 @@ PythonOpsResolver::PythonOpsResolver() {
   AddSquaredDifference();
   AddSqueeze();
   AddStridedSlice();
-  AddStacker();
+  //AddStacker();
   AddSub();
   AddSum();
   AddSvdf();
@@ -130,7 +130,7 @@ PythonOpsResolver::PythonOpsResolver() {
   AddUnpack();
   AddVarHandle();
   AddWhile();
-  AddWindow();
+  //AddWindow();
   AddZerosLike();
 }
 
diff --git a/tensorflow/lite/core/c/common.h b/tensorflow/lite/core/c/common.h
index d9abf9ea..b7a85d32 100644
--- a/tensorflow/lite/core/c/common.h
+++ b/tensorflow/lite/core/c/common.h
@@ -570,13 +570,6 @@ typedef struct TfLiteNode {
 // - name
 // - sparsity
 typedef struct TfLiteTensor {
-  // TODO(b/155784997): Consider consolidating these quantization fields:
-  // Quantization information. Replaces params field above.
-  TfLiteQuantization quantization;
-
-  // Quantization information.
-  TfLiteQuantizationParams params;
-
   // A union of data pointers. The appropriate type should be used for a typed
   // tensor based on `type`.
   TfLitePtrUnion data;
@@ -586,16 +579,23 @@ typedef struct TfLiteTensor {
   // and the element datatype size should be equal to `bytes` below.
   TfLiteIntArray* dims;
 
+  // The data type specification for data stored in `data`. This affects
+  // what member of `data` union should be used.
+  TfLiteType type;
+
+  // TODO(b/155784997): Consider consolidating these quantization fields:
+  // Quantization information. Replaces params field above.
+  TfLiteQuantization quantization;
+
+  // Quantization information.
+  TfLiteQuantizationParams params;
+
   // The number of bytes required to store the data of this Tensor. I.e.
   // (bytes of each element) * dims[0] * ... * dims[n-1].  For example, if
   // type is kTfLiteFloat32 and dims = {3, 2} then
   // bytes = sizeof(float) * 3 * 2 = 4 * 3 * 2 = 24.
   size_t bytes;
 
-  // The data type specification for data stored in `data`. This affects
-  // what member of `data` union should be used.
-  TfLiteType type;
-
   // How memory is mapped
   //  kTfLiteMmapRo: Memory mapped read only.
   //  i.e. weights
@@ -621,10 +621,6 @@ typedef struct TfLiteNode {
   // Outputs to this node expressed as indices into the simulator's tensors.
   TfLiteIntArray* outputs;
 
-  // intermediate tensors to this node expressed as indices into the simulator's
-  // tensors.
-  TfLiteIntArray* intermediates;
-
   // Opaque data provided by the node implementer through `Registration.init`.
   void* user_data;
 
@@ -634,7 +630,6 @@ typedef struct TfLiteNode {
 
   // Custom initial data. This is the opaque data provided in the flatbuffer.
   // WARNING: This is an experimental interface that is subject to change.
-  const void* custom_initial_data;
   int custom_initial_data_size;
 } TfLiteNode;
 #endif  // TF_LITE_STATIC_MEMORY
diff --git a/tensorflow/lite/kernels/internal/quantization_util.cc b/tensorflow/lite/kernels/internal/quantization_util.cc
index 62045d67..5ad1e885 100644
--- a/tensorflow/lite/kernels/internal/quantization_util.cc
+++ b/tensorflow/lite/kernels/internal/quantization_util.cc
@@ -314,8 +314,13 @@ void PreprocessSoftmaxScaling(double beta, double input_scale,
                        max_real_multiplier);
 #endif  // TFLITE_EMULATE_FLOAT
 
-  QuantizeMultiplierGreaterThanOne(input_beta_real_multiplier,
-                                   quantized_multiplier, left_shift);
+  if(input_beta_real_multiplier > 1.) {
+    QuantizeMultiplierGreaterThanOne(input_beta_real_multiplier,
+                                    quantized_multiplier, left_shift);
+  } else {
+    QuantizeMultiplierSmallerThanOneExp(input_beta_real_multiplier,
+                                    quantized_multiplier, left_shift);
+  }
 }
 
 void PreprocessLogSoftmaxScalingExp(double beta, double input_scale,
diff --git a/tensorflow/lite/micro/kernels/activations.cc b/tensorflow/lite/micro/kernels/activations.cc
index 1086325c..3425bb2a 100644
--- a/tensorflow/lite/micro/kernels/activations.cc
+++ b/tensorflow/lite/micro/kernels/activations.cc
@@ -28,6 +28,25 @@ limitations under the License.
 #include "tensorflow/lite/micro/micro_utils.h"
 
 namespace tflite {
+
+template <typename T>
+void ReluQuantized(const ReluOpData& data, const RuntimeShape& input_shape,
+                   const RuntimeShape& output_shape, const T* input_data,
+                   T* output_data) {
+  const int flat_size = MatchingFlatSize(input_shape, output_shape);
+  for (int i = 0; i < flat_size; ++i) {
+    const int32_t val = static_cast<int32_t>(input_data[i]);
+    int32_t clamped =
+        data.params.output_offset +
+        MultiplyByQuantizedMultiplier(val - data.params.input_offset,
+                                      data.params.output_multiplier,
+                                      data.params.output_shift);
+    clamped = std::max(data.params.quantized_activation_min, clamped);
+    clamped = std::min(data.params.quantized_activation_max, clamped);
+    output_data[i] = static_cast<T>(clamped);
+  }
+}
+
 namespace {
 
 void* ReluInit(TfLiteContext* context, const char* buffer, size_t length) {
@@ -53,6 +72,13 @@ TfLiteStatus ReluEval(TfLiteContext* context, TfLiteNode* node) {
 
       return kTfLiteOk;
     }
+    case kTfLiteInt16: {
+      tflite::ReluQuantized<int16_t>(data, tflite::micro::GetTensorShape(input),
+                            tflite::micro::GetTensorShape(output),
+                            tflite::micro::GetTensorData<int16_t>(input),
+                            tflite::micro::GetTensorData<int16_t>(output));
+      return kTfLiteOk;
+    }
     case kTfLiteInt8: {
       tflite::ReluQuantized(data, tflite::micro::GetTensorShape(input),
                             tflite::micro::GetTensorShape(output),
diff --git a/tensorflow/lite/micro/kernels/activations.h b/tensorflow/lite/micro/kernels/activations.h
index e953f0e0..8ad0a567 100644
--- a/tensorflow/lite/micro/kernels/activations.h
+++ b/tensorflow/lite/micro/kernels/activations.h
@@ -36,9 +36,10 @@ struct Relu6OpData {
   int8_t zero_int8;
 };
 
+template <typename T>
 void ReluQuantized(const ReluOpData& data, const RuntimeShape& input_shape,
-                   const RuntimeShape& output_shape, const int8_t* input_data,
-                   int8_t* output_data);
+                   const RuntimeShape& output_shape, const T* input_data,
+                   T* output_data);
 
 template <typename T>
 void CalculateReluOpData(const TfLiteTensor* input, TfLiteTensor* output,
diff --git a/tensorflow/lite/micro/kernels/activations_common.cc b/tensorflow/lite/micro/kernels/activations_common.cc
index 2ec3a1bf..2a724f61 100644
--- a/tensorflow/lite/micro/kernels/activations_common.cc
+++ b/tensorflow/lite/micro/kernels/activations_common.cc
@@ -33,23 +33,6 @@ namespace tflite {
 const int kActivationsInputTensor = 0;
 const int kActivationsOutputTensor = 0;
 
-void ReluQuantized(const ReluOpData& data, const RuntimeShape& input_shape,
-                   const RuntimeShape& output_shape, const int8_t* input_data,
-                   int8_t* output_data) {
-  const int flat_size = MatchingFlatSize(input_shape, output_shape);
-  for (int i = 0; i < flat_size; ++i) {
-    const int32_t val = static_cast<int32_t>(input_data[i]);
-    int32_t clamped =
-        data.params.output_offset +
-        MultiplyByQuantizedMultiplier(val - data.params.input_offset,
-                                      data.params.output_multiplier,
-                                      data.params.output_shift);
-    clamped = std::max(data.params.quantized_activation_min, clamped);
-    clamped = std::min(data.params.quantized_activation_max, clamped);
-    output_data[i] = static_cast<int8_t>(clamped);
-  }
-}
-
 template <typename T>
 void CalculateReluOpData(const TfLiteTensor* input, TfLiteTensor* output,
                          ReluOpData* data) {
@@ -127,6 +110,8 @@ TfLiteStatus ReluPrepare(TfLiteContext* context, TfLiteNode* node) {
 
   if (input->type == kTfLiteInt8) {
     CalculateReluOpData<int8_t>(input, output, data);
+  } else if (input->type == kTfLiteInt16) {
+    CalculateReluOpData<int16_t>(input, output, data);
   }
 
   micro_context->DeallocateTempTfLiteTensor(input);
diff --git a/tensorflow/lite/micro/kernels/broadcast_to.cc b/tensorflow/lite/micro/kernels/broadcast_to.cc
index 61deaa31..bfe951e1 100644
--- a/tensorflow/lite/micro/kernels/broadcast_to.cc
+++ b/tensorflow/lite/micro/kernels/broadcast_to.cc
@@ -68,8 +68,10 @@ TfLiteStatus ValidateOutputTensor(TfLiteContext* context, TfLiteTensor* input,
   // Validating the shape of the output tensor.
   tflite::RuntimeShape output_shape = tflite::GetTensorShape(output);
   for (int idx = 0; idx < output_num_dims; ++idx) {
+    printf("idx = %d, output shape dim = %d, shape dim = %d\n", idx, output_shape.Dims(idx), get_shape_data(idx));
     TF_LITE_ENSURE(context, output_shape.Dims(idx) == get_shape_data(idx));
   }
+  printf("\n");
   return kTfLiteOk;
 }
 
diff --git a/tensorflow/lite/micro/kernels/concatenation.cc b/tensorflow/lite/micro/kernels/concatenation.cc
index b4a838f7..e9f0381e 100644
--- a/tensorflow/lite/micro/kernels/concatenation.cc
+++ b/tensorflow/lite/micro/kernels/concatenation.cc
@@ -28,7 +28,7 @@ namespace tflite {
 
 namespace {
 
-constexpr int kMaxInputNum = 10;  // Maximum number of input tensors
+constexpr int kMaxInputNum = 40;  // Maximum number of input tensors
 constexpr int kOutputTensor = 0;
 
 struct OpData {
diff --git a/tensorflow/lite/micro/kernels/fully_connected.cc b/tensorflow/lite/micro/kernels/fully_connected.cc
index 54576faf..e04a1906 100644
--- a/tensorflow/lite/micro/kernels/fully_connected.cc
+++ b/tensorflow/lite/micro/kernels/fully_connected.cc
@@ -174,7 +174,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
               tflite::micro::GetTensorShape(filter),
               tflite::micro::GetTensorData<int8_t>(filter),
               tflite::micro::GetTensorShape(bias),
-              tflite::micro::GetOptionalTensorData<int64_t>(bias),
+              tflite::micro::GetOptionalTensorData<int32_t>(bias),
               tflite::micro::GetTensorShape(output),
               tflite::micro::GetTensorData<int16_t>(output));
           break;
diff --git a/tensorflow/lite/micro/kernels/pack.cc b/tensorflow/lite/micro/kernels/pack.cc
index 7b4aeef2..3f1c874b 100644
--- a/tensorflow/lite/micro/kernels/pack.cc
+++ b/tensorflow/lite/micro/kernels/pack.cc
@@ -85,6 +85,10 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
       return PackImpl<int8_t>(context, node, output, data->values_count,
                               data->axis);
     }
+    case kTfLiteInt16: {
+      return PackImpl<int16_t>(context, node, output, data->values_count,
+                              data->axis);
+    }
     case kTfLiteInt32: {
       return PackImpl<int32_t>(context, node, output, data->values_count,
                                data->axis);
diff --git a/tensorflow/lite/micro/kernels/transpose.cc b/tensorflow/lite/micro/kernels/transpose.cc
index 710bfca4..82f0d4ed 100644
--- a/tensorflow/lite/micro/kernels/transpose.cc
+++ b/tensorflow/lite/micro/kernels/transpose.cc
@@ -97,6 +97,12 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
                                tflite::micro::GetTensorShape(output),
                                tflite::micro::GetTensorData<float>(output));
       break;
+    case kTfLiteInt16:
+      reference_ops::Transpose(params, tflite::micro::GetTensorShape(input),
+                               tflite::micro::GetTensorData<int16_t>(input),
+                               tflite::micro::GetTensorShape(output),
+                               tflite::micro::GetTensorData<int16_t>(output));
+      break;
     case kTfLiteInt8:
       reference_ops::Transpose(params, tflite::micro::GetTensorShape(input),
                                tflite::micro::GetTensorData<int8_t>(input),
diff --git a/tensorflow/lite/micro/kernels/transpose_conv.cc b/tensorflow/lite/micro/kernels/transpose_conv.cc
index a2ac2b46..89b4206a 100644
--- a/tensorflow/lite/micro/kernels/transpose_conv.cc
+++ b/tensorflow/lite/micro/kernels/transpose_conv.cc
@@ -123,7 +123,7 @@ TfLiteStatus CalculateOpData(TfLiteContext* context, TfLiteNode* node,
     if (input->type == kTfLiteInt16) {
       TFLITE_DCHECK(filter->type == kTfLiteInt8);
       TFLITE_DCHECK(output->type == kTfLiteInt16);
-      if (bias->type == kTfLiteInt16) {
+      if (bias && (bias->type == kTfLiteInt16 || bias->type == kTfLiteInt32)) {
         TFLITE_DCHECK(
             context->RequestScratchBufferInArena(
                 context, GetTensorShape(bias).FlatSize() * sizeof(std::int64_t),
@@ -320,6 +320,24 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
             tflite::micro::GetTensorShape(output),
             tflite::micro::GetTensorData<int16_t>(output),
             tflite::micro::GetTensorShape(nullptr), nullptr, scratch_buffer);
+      } else if (bias != nullptr && bias->type == kTfLiteInt32) {
+        std::int64_t* bias_converted_buffer =
+            static_cast<int64_t*>(context->GetScratchBuffer(
+                context, data.bias_converted_buffer_index));
+        for (int i = 0; i < tflite::micro::GetTensorShape(bias).FlatSize();
+             i++) {
+          bias_converted_buffer[i] = bias->data.i32[i];
+        }
+        reference_integer_ops::TransposeConv(
+            data.params, data.per_channel_output_multiplier,
+            data.per_channel_output_shift, tflite::micro::GetTensorShape(input),
+            tflite::micro::GetTensorData<int16_t>(input),
+            tflite::micro::GetTensorShape(filter),
+            tflite::micro::GetTensorData<int8_t>(filter),
+            tflite::micro::GetTensorShape(bias), bias_converted_buffer,
+            tflite::micro::GetTensorShape(output),
+            tflite::micro::GetTensorData<int16_t>(output),
+            tflite::micro::GetTensorShape(nullptr), nullptr, scratch_buffer);
       } else {
         reference_integer_ops::TransposeConv(
             data.params, data.per_channel_output_multiplier,
diff --git a/tensorflow/lite/micro/kernels/unpack.cc b/tensorflow/lite/micro/kernels/unpack.cc
index 3ce4c33f..aaa938b3 100644
--- a/tensorflow/lite/micro/kernels/unpack.cc
+++ b/tensorflow/lite/micro/kernels/unpack.cc
@@ -86,6 +86,9 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
     case kTfLiteInt32: {
       return UnpackImpl<int32_t>(context, node, input, data->num, data->axis);
     }
+    case kTfLiteInt16: {
+      return UnpackImpl<int16_t>(context, node, input, data->num, data->axis);
+    }
     case kTfLiteInt8: {
       return UnpackImpl<int8_t>(context, node, input, data->num, data->axis);
     }
diff --git a/tensorflow/lite/micro/memory_planner/greedy_memory_planner.h b/tensorflow/lite/micro/memory_planner/greedy_memory_planner.h
index b2cdb617..a79e7974 100644
--- a/tensorflow/lite/micro/memory_planner/greedy_memory_planner.h
+++ b/tensorflow/lite/micro/memory_planner/greedy_memory_planner.h
@@ -112,6 +112,8 @@ class GreedyMemoryPlanner : public MicroMemoryPlanner {
   // that aren't being used during a phase of invocation are overwritten.
   bool preserves_all_tensors() const override { return false; }
 
+  TF_LITE_REMOVE_VIRTUAL_DELETE
+
  private:
   // Whether a buffer is active in a given time range.
   bool DoesEntryOverlapInTime(const ListEntry* entry, const int first_time_used,
@@ -161,8 +163,6 @@ class GreedyMemoryPlanner : public MicroMemoryPlanner {
 
   // Whether buffers have been added since the last plan was calculated.
   bool need_to_calculate_offsets_;
-
-  TF_LITE_REMOVE_VIRTUAL_DELETE
 };
 
 }  // namespace tflite
diff --git a/tensorflow/lite/micro/micro_allocator.h b/tensorflow/lite/micro/micro_allocator.h
index 4eff167d..a29d56ed 100644
--- a/tensorflow/lite/micro/micro_allocator.h
+++ b/tensorflow/lite/micro/micro_allocator.h
@@ -250,6 +250,14 @@ class MicroAllocator {
 
   TfLiteBridgeBuiltinDataAllocator* GetBuiltinDataAllocator();
 
+  MicroMemoryPlanner* memory_planner() { return memory_planner_;}
+
+  // Returns the pointer for the array of ScratchBufferRequest allocations in
+  // the head section.
+  internal::ScratchBufferRequest* GetScratchBufferRequests();
+
+  size_t GetScratchBufferRequestCount() { return scratch_buffer_request_count_;}
+
  protected:
   MicroAllocator(SingleArenaBufferAllocator* memory_allocator,
                  MicroMemoryPlanner* memory_planner);
@@ -313,10 +321,6 @@ class MicroAllocator {
   // preparing.
   TfLiteStatus InitScratchBufferData();
 
-  // Returns the pointer for the array of ScratchBufferRequest allocations in
-  // the head section.
-  internal::ScratchBufferRequest* GetScratchBufferRequests();
-
   // A simple memory allocator that always allocate from the arena tail or head.
   INonPersistentBufferAllocator* non_persistent_buffer_allocator_;
   IPersistentBufferAllocator* persistent_buffer_allocator_;
diff --git a/tensorflow/lite/micro/micro_context.h b/tensorflow/lite/micro/micro_context.h
index 2dd3233a..d4b1eb5e 100644
--- a/tensorflow/lite/micro/micro_context.h
+++ b/tensorflow/lite/micro/micro_context.h
@@ -19,6 +19,27 @@ limitations under the License.
 #include "tensorflow/lite/c/common.h"
 #include "tensorflow/lite/micro/micro_graph.h"
 
+#define XCORE_TFLITE_MICRO_PATCHED
+
+#ifdef NO_INTERPRETER
+
+namespace tflite {
+  const TfLiteStatus kTfLiteAbort = static_cast<TfLiteStatus>(15);
+
+  struct MicroContext{
+      TfLiteTensor* (*AllocateTempInputTensor)(const TfLiteNode* node, int index);
+      TfLiteTensor* (*AllocateTempOutputTensor)(const TfLiteNode* node, int index);
+      void (*DeallocateTempTfLiteTensor)(TfLiteTensor* tensor);
+      void* (*external_context)();
+      MicroGraph& (*graph)();
+  };
+  static inline MicroContext* GetMicroContext(const struct TfLiteContext* context){
+      return reinterpret_cast<MicroContext*>(context->impl_);
+  }
+}
+
+#else
+
 namespace tflite {
 // TODO(b/149795762): kTfLiteAbort cannot be part of the tflite TfLiteStatus.
 const TfLiteStatus kTfLiteAbort = static_cast<TfLiteStatus>(15);
@@ -52,6 +73,7 @@ class MicroContext {
 
   // Returns a temporary TfLiteTensor struct for a given index.
   virtual TfLiteTensor* AllocateTempTfLiteTensor(int tensor_idx) = 0;
+  virtual TfLiteTensor* AllocateTempTfLiteTensor(int tensor_idx, int sg){return nullptr;}
 
   // Returns a temporary TfLiteTensor struct for the specified input tensor of a
   // given mode. This is the recommended API over the deprecated
@@ -85,6 +107,7 @@ class MicroContext {
 
   // Returns a TfLiteEvalTensor struct for a given index.
   virtual TfLiteEvalTensor* GetEvalTensor(int tensor_idx) = 0;
+  virtual TfLiteEvalTensor* GetEvalTensor(int tensor_idx, int sg){return nullptr;}
 
   // Does not take ownership of the pointer and the pointer must refer to valid
   // an object that outlive this class instance.
@@ -124,10 +147,18 @@ inline TfLiteTensor* MicroContextGetTensor(const struct TfLiteContext* context,
                                            int tensor_idx) {
   return GetMicroContext(context)->AllocateTempTfLiteTensor(tensor_idx);
 }
+inline TfLiteTensor* MicroContextGetTensor(const struct TfLiteContext* context,
+                                           int tensor_idx, int sg) {
+  return GetMicroContext(context)->AllocateTempTfLiteTensor(tensor_idx, sg);
+}
 inline TfLiteEvalTensor* MicroContextGetEvalTensor(
     const struct TfLiteContext* context, int tensor_idx) {
   return GetMicroContext(context)->GetEvalTensor(tensor_idx);
 }
+inline TfLiteEvalTensor* MicroContextGetEvalTensor(
+    const struct TfLiteContext* context, int tensor_idx, int sg) {
+  return GetMicroContext(context)->GetEvalTensor(tensor_idx, sg);
+}
 inline TfLiteExternalContext* MicroContextGetExternalContext(
     TfLiteContext* context, TfLiteExternalContextType unused) {
   return reinterpret_cast<TfLiteExternalContext*>(
@@ -140,4 +171,6 @@ void MicroContextReportOpError(struct TfLiteContext* context,
 
 }  // namespace tflite
 
+#endif  // NO_INTERPRETER
+
 #endif  // TENSORFLOW_LITE_MICRO_MICRO_CONTEXT_H_
diff --git a/tensorflow/lite/micro/micro_graph.h b/tensorflow/lite/micro/micro_graph.h
index 79b36496..3b880252 100644
--- a/tensorflow/lite/micro/micro_graph.h
+++ b/tensorflow/lite/micro/micro_graph.h
@@ -20,6 +20,21 @@ limitations under the License.
 #include "tensorflow/lite/micro/micro_common.h"
 #include "tensorflow/lite/micro/micro_resource_variable.h"
 
+#ifdef NO_INTERPRETER
+
+namespace tflite {
+  struct MicroGraph{
+      int (*NumSubgraphs)();
+      size_t (*NumSubgraphInputs)(int subgraph_idx);
+      size_t (*NumSubgraphOutputs)(int subgraph_idx);
+      TfLiteEvalTensor* (*GetSubgraphInput)(int subgraph_idx, int i);
+      TfLiteEvalTensor* (*GetSubgraphOutput)(int subgraph_idx, int i);
+      TfLiteStatus (*InvokeSubgraph)(int subgraph_idx);
+  };
+}
+
+#else
+
 namespace tflite {
 
 // Abstracts the details of interacting with the graph from the kernels
@@ -59,4 +74,6 @@ class MicroGraph {
 
 }  // namespace tflite
 
+#endif  // NO_INTERPRETER
+
 #endif  // TENSORFLOW_LITE_MICRO_MICRO_GRAPH_H_
diff --git a/tensorflow/lite/micro/micro_interpreter.h b/tensorflow/lite/micro/micro_interpreter.h
index 1c419962..a74d4d0f 100644
--- a/tensorflow/lite/micro/micro_interpreter.h
+++ b/tensorflow/lite/micro/micro_interpreter.h
@@ -146,6 +146,13 @@ class MicroInterpreter {
     return allocator_.preserves_all_tensor();
   }
 
+  size_t operators_size(int sg) const { return model_->subgraphs()->Get(sg)->operators()->size(); }
+
+  // For debugging only.
+  const NodeAndRegistration node_and_registration(int node_index, int sg)  {
+    return graph_.GetAllocations()[sg].node_and_registrations[node_index];
+  }
+
  protected:
   const MicroAllocator& allocator() const { return allocator_; }
   const TfLiteContext& context() const { return context_; }
diff --git a/tensorflow/lite/micro/micro_interpreter_context.cc b/tensorflow/lite/micro/micro_interpreter_context.cc
index 098df15d..e702d494 100644
--- a/tensorflow/lite/micro/micro_interpreter_context.cc
+++ b/tensorflow/lite/micro/micro_interpreter_context.cc
@@ -56,6 +56,12 @@ TfLiteTensor* MicroInterpreterContext::AllocateTempTfLiteTensor(
                                              graph_.GetCurrentSubgraphIndex());
 }
 
+TfLiteTensor* MicroInterpreterContext::AllocateTempTfLiteTensor(int tensor_idx, int sg) {
+  return allocator_.AllocateTempTfLiteTensor(model_, graph_.GetAllocations(),
+                                             tensor_idx,
+                                             sg);
+}
+
 void MicroInterpreterContext::DeallocateTempTfLiteTensor(TfLiteTensor* tensor) {
   return allocator_.DeallocateTempTfLiteTensor(tensor);
 }
@@ -76,6 +82,11 @@ TfLiteEvalTensor* MicroInterpreterContext::GetEvalTensor(int tensor_idx) {
               .tensors[tensor_idx];
 }
 
+TfLiteEvalTensor* MicroInterpreterContext::GetEvalTensor(int tensor_idx, int sg) {
+  return &graph_.GetAllocations()[sg]
+              .tensors[tensor_idx];
+}
+
 void MicroInterpreterContext::SetScratchBufferHandles(
     ScratchBufferHandle* scratch_buffer_handles) {
   scratch_buffer_handles_ = scratch_buffer_handles;
@@ -83,7 +94,8 @@ void MicroInterpreterContext::SetScratchBufferHandles(
 
 TfLiteStatus MicroInterpreterContext::set_external_context(
     void* external_context_payload) {
-  TFLITE_DCHECK(state_ == InterpreterState::kPrepare ||
+  TFLITE_DCHECK(state_ == InterpreterState::kInit ||
+                state_ == InterpreterState::kPrepare ||
                 state_ == InterpreterState::kInvoke);
   if (external_context_payload == nullptr ||
       external_context_payload_ != nullptr) {
diff --git a/tensorflow/lite/micro/micro_interpreter_context.h b/tensorflow/lite/micro/micro_interpreter_context.h
index 5986dc37..53de3cc9 100644
--- a/tensorflow/lite/micro/micro_interpreter_context.h
+++ b/tensorflow/lite/micro/micro_interpreter_context.h
@@ -67,6 +67,7 @@ class MicroInterpreterContext : public MicroContext {
   // Returns a temporary TfLiteTensor struct for a given index.
   // Virtual so that it can be faked for kernel tests.
   virtual TfLiteTensor* AllocateTempTfLiteTensor(int tensor_idx) override;
+  virtual TfLiteTensor* AllocateTempTfLiteTensor(int tensor_idx, int sg) override;
 
   // Deallocates a temp TfLiteTensor.
   // Virtual so that it can be faked for kernel tests.
@@ -85,6 +86,7 @@ class MicroInterpreterContext : public MicroContext {
   // Returns a TfLiteEvalTensor struct for a given index.
   // Virtual so that it can be faked for kernel tests.
   virtual TfLiteEvalTensor* GetEvalTensor(int tensor_idx) override;
+  virtual TfLiteEvalTensor* GetEvalTensor(int tensor_idx, int sg) override;
 
   // Sets the State of MemoryPlanning MicroInterpreterContext
   void SetInterpreterState(InterpreterState state);
diff --git a/tensorflow/lite/micro/test_helpers.cc b/tensorflow/lite/micro/test_helpers.cc
index 3f0f5ec0..00b9541a 100644
--- a/tensorflow/lite/micro/test_helpers.cc
+++ b/tensorflow/lite/micro/test_helpers.cc
@@ -442,11 +442,19 @@ const Model* BuildModelWithUnusedOperatorOutputs() {
           *builder, builder->CreateVector(tensor_shape, tensor_shape_size),
           TensorType_INT8, 0,
           builder->CreateString("test_unused_output_tensor"), 0, false)};
+#ifdef _MSC_VER
+  constexpr size_t inputs_size = 1;
+#else
   constexpr size_t inputs_size = 0;
+#endif
   const int32_t inputs[inputs_size] = {};
   constexpr size_t outputs_size = 1;
   const int32_t outputs[outputs_size] = {0};
-  constexpr size_t operator_inputs_size = 0;
+#ifdef _MSC_VER
+  constexpr size_t operator_inputs_size = 1;
+#else
+   constexpr size_t operator_inputs_size = 0;
+#endif
   const int32_t operator_inputs[operator_inputs_size] = {};
   constexpr size_t operator_outputs_size = 2;
   const int32_t operator_outputs[operator_outputs_size] = {0, 1};
diff --git a/tensorflow/lite/micro/tflite_bridge/micro_error_reporter.h b/tensorflow/lite/micro/tflite_bridge/micro_error_reporter.h
index d3702f46..186a226e 100644
--- a/tensorflow/lite/micro/tflite_bridge/micro_error_reporter.h
+++ b/tensorflow/lite/micro/tflite_bridge/micro_error_reporter.h
@@ -28,7 +28,6 @@ class MicroErrorReporter : public ErrorReporter {
   ~MicroErrorReporter() override {}
   int Report(const char* format, va_list args) override;
 
- private:
   TF_LITE_REMOVE_VIRTUAL_DELETE
 };
 
